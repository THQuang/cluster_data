{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import numpy as np\n",
    "def pre_process(path, tasks):\n",
    "    '''\n",
    "    Đọc dữ liệu từ mỗi file csv tương ứng với mỗi task, lấy ra chuỗi smiles và labels của chúng trong từng task. Sau đó \n",
    "    đánh nhãn cho mỗi chuỗi smiles nếu như chuỗi smiles không tồn tại trong một số task thì nhãn của chúng không xác định được lấy giá trị=6 \n",
    "\n",
    "    Input\n",
    "        path: folder chứa file dữ liệu của từng task \n",
    "        tasks: danh sách tên của các task \n",
    "    Output\n",
    "        all_smiles: danh sách chuỗi smiles \n",
    "        list_labels: danh sách nhãn của mỗi task ứng chuỗi smiles tương ứng\n",
    "    '''\n",
    "    # Get smiles and labels for each task \n",
    "    task_smiles = []\n",
    "    task_labels = []\n",
    "    all_smiles  = []\n",
    "    for task in tasks:\n",
    "        path_task = path + \"/refined_merged_{}.csv\".format(task)\n",
    "        data      = pd.read_csv(path_task)\n",
    "        smiles    = data['SMILES'].tolist()\n",
    "        label     = data['Label'].tolist()\n",
    "        task_smiles.append(smiles)\n",
    "        task_labels.append(label)\n",
    "        all_smiles.extend(smiles)\n",
    "\n",
    "    # labeling for all smiles \n",
    "    all_smiles  = list(set(all_smiles))\n",
    "    list_labels = []\n",
    "    for smiles in all_smiles:\n",
    "        labels = []\n",
    "        for i in range(len(tasks)):\n",
    "            if smiles in task_smiles[i]:\n",
    "                idx = task_smiles[i].index(smiles)\n",
    "                labels.append(task_labels[i][idx])\n",
    "            else:\n",
    "                # smiles in not labeled in this task\n",
    "                labels.append(np.nan)\n",
    "        list_labels.append(labels)\n",
    "    return all_smiles, list_labels\n",
    "\n",
    "tasks = ['BRE','CNS','COL','LEU','LNS','MEL','OVA','PRO','REN']\n",
    "all_smiles, list_labels = pre_process('./raw_data', tasks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature MP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_csv('./df_mf.csv')\n",
    "\n",
    "vector = np.array(data)\n",
    "feature   = vector[::,2:]\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "data_normal = scaler.fit_transform(feature)\n",
    "\n",
    "# Mapping smiles and labels\n",
    "smiles = data['SMILES'].tolist()\n",
    "\n",
    "list_index = []\n",
    "for smile in smiles:\n",
    "    idx = all_smiles.index(smile)\n",
    "    list_index.append(idx)\n",
    "all_smiles = np.array(all_smiles)[list_index]\n",
    "list_labels = np.array(list_labels)[list_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5978\n",
      "7160\n",
      "3222\n",
      "6958\n",
      "6578\n",
      "3199\n",
      "4867\n",
      "4896\n",
      "3853\n"
     ]
    }
   ],
   "source": [
    "from sklearn_som.som import SOM\n",
    "m = 3\n",
    "n = 3\n",
    "\n",
    "som = SOM(m=m, n=n, dim=data_normal.shape[1], random_state=24)\n",
    "# Fit it to the data\n",
    "som.fit(data_normal)\n",
    "\n",
    "# Assign each datapoint to its predicted cluster\n",
    "predictions = som.predict(data_normal)\n",
    "\n",
    "list_len_sample = []\n",
    "list_index = []\n",
    "for i in range(m*n): \n",
    "    list_len_sample.append(len(np.where(predictions == i)[0]))\n",
    "    list_index.append(np.where(predictions == i)[0])\n",
    "    print(len(np.where(predictions == i)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual check num of sample in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:      num_sample: 5978       : num_sample for each task: [5657, 5712, 5750, 5335, 5779, 5771, 5790, 5457, 5779]\n",
      "Cluster 1:      num_sample: 7160       : num_sample for each task: [6715, 6794, 6760, 6267, 6871, 6902, 6935, 6453, 6864]\n",
      "Cluster 2:      num_sample: 3222       : num_sample for each task: [2728, 2833, 2900, 2406, 2919, 2936, 3012, 2573, 2935]\n",
      "Cluster 3:      num_sample: 6958       : num_sample for each task: [6538, 6602, 6681, 6072, 6675, 6709, 6758, 6294, 6734]\n",
      "Cluster 4:      num_sample: 6578       : num_sample for each task: [5849, 5983, 6091, 5316, 6100, 6189, 6213, 5662, 6125]\n",
      "Cluster 5:      num_sample: 3199       : num_sample for each task: [2601, 2694, 2799, 2260, 2777, 2888, 2919, 2399, 2833]\n",
      "Cluster 6:      num_sample: 4867       : num_sample for each task: [4398, 4490, 4577, 4068, 4560, 4600, 4629, 4196, 4580]\n",
      "Cluster 7:      num_sample: 4896       : num_sample for each task: [4258, 4399, 4513, 3817, 4461, 4565, 4606, 4054, 4530]\n",
      "Cluster 8:      num_sample: 3853       : num_sample for each task: [3038, 3186, 3467, 2607, 3371, 3457, 3532, 2865, 3398]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_cluster = [[] for i in range(len(list_index))]\n",
    "for i in range(len(list_index)):\n",
    "    data_cluster[i] = list_labels[list_index[i]]\n",
    "\n",
    "def get_data_task(sample, list_index):\n",
    "    num_sample_task = []\n",
    "    for i in range(len(list_index)):\n",
    "        task_data = np.where((sample.transpose()[i] == 0) | (sample.transpose()[i] == 1))[0]\n",
    "        num_sample_task.append(len(task_data))\n",
    "    return num_sample_task\n",
    "\n",
    "for idx, sample in enumerate(data_cluster):\n",
    "    num_sample_task = get_data_task(sample, list_index)\n",
    "    print(f\"Cluster {idx}:      num_sample: {len(sample)}       : num_sample for each task: {num_sample_task}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36659\n",
      "4074\n",
      "5978\n",
      "(5978, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_cluster_idx = 0\n",
    "# get index test dataset \n",
    "idx_test = list_index[test_cluster_idx]\n",
    "\n",
    "# Split train_val with 90/10\n",
    "idx_train = [i  for i in range(len(all_smiles)) if i not in idx_test]\n",
    "idx_train, idx_val = train_test_split(idx_train, test_size=0.10, random_state=2)\n",
    "\n",
    "print(len(idx_train))\n",
    "print(len(idx_val))\n",
    "print(len(idx_test))\n",
    "\n",
    "print(list_labels[idx_test].shape)\n",
    "\n",
    "# Save index split data\n",
    "# np.save('./cluster_split/index_train_{}.npy'.format(test_cluster_idx),idx_train_1)\n",
    "# np.save('./cluster_split/index_val_{}.npy'.format(test_cluster_idx),idx_val_1)\n",
    "# np.save('./cluster_split/index_test_{}.npy'.format(test_cluster_idx),idx_test)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2a409bfd8a069a0f96eb5da09c1bf8fd3c3231cdc8a43835426a840cf4dc365"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
